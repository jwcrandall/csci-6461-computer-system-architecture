\documentclass[fontsize=4pt]{scrartcl}
\usepackage{lmodern}
%\documentclass[10pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage[margin=0.25in]{geometry}        % See geometry.pdf to learn the layout options. There are lots.
%\geometry{letterpaper}                     % ... or a4paper or a5paper or ... 
\geometry{landscape}                        % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}                       % Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{esint}                          %for cyclic integrals
\usepackage{ amssymb } 
\usepackage[usenames, dvipsnames]{color}
\usepackage{multicol}
\usepackage{color,soul}
\usepackage{siunitx}                        % Scientific Notation
\usepackage{graphicx}

\def\rcurs{{\mbox{$\resizebox{.16in}{.08in}{\includegraphics{ScriptR}}$}}}
\def\brcurs{{\mbox{$\resizebox{.16in}{.08in}{\includegraphics{BoldR}}$}}}
\def\hrcurs{{\mbox{$\hat \brcurs$}}}

\begin{document}
\colorbox{YellowGreen}{Joe Crandall's CSCI 6461 Computer System Architecture}
\colorbox{Thistle}{Used Heavily}
\colorbox{Cyan}{Topic}
\colorbox{Orange}{SubTopic}
\colorbox{Aquamarine}{KNOWTHISMATH}
\colorbox{RubineRed}{Definition/Constant/Units}
\colorbox{Yellow}{break}
\colorbox{Cyan}{0518 Lecture 1 Introduction} 
Computer Architecture: Functional operation of the individual HW units within a computer system, and the flow of information and control among them.
\hl{I} 
Program Counter (PC): address of next instruction to be executed
\hl{I}
Condition Code (CC): set when arithmetic/logical operations are executed, four 1 bit elements, Overflow, Underflow, Divzero, EqualOrNot
\hl{I}
Instruction Register (IR): holds instruction to be executed
\hl{I}
Memory Address Register (MAR): holds the address of the word  to be fetched from memory
\hl{I}
Memory Buffer Register (MBR): holds the word just fetched from or the word to be last stored into memory
\hl{I}
general purpose registers (GPR0$\ldots$GPR3): mnemonically referred to, may be used as accumulators.
\hl{I}
machine fault register (MFR): contains the ID code of a machine fault after it occurs
\hl{I}
index registers: (X1$\ldots$X3): contains a base address that supports register addressing of memory.
\hl{I}
Instruction execution cyle: instruction fetch - obtain instruction from program storage, instruction decode - determine requiered actions and instruction size, operand fetch - locate and obtain operand data, execute - compute result value or status, result store - deposit results in storage for later use, next instruction - deterine sucessor instruction.
\colorbox{Cyan}{0523 Lecture 2 Basic System Design}
The simplest shifter is the shift register, which can shift by one position per clock cycle
\hl{I}
Performance, Resposne Time, Throughput, Execution Times
\hl{I}
Instruction Set Architecture, Processors, Memory, I/O Systems
\hl{I}
Finding the absolute value of an integer: abs - str	r0,0,$<$tempInt$>$; store r0 in $<$tempInt$>$, some location - ldr	r1,0,smask; mask for sign bit = 100 000 000 000 000 000 - and r1,r0; AND r1 and r0: if r0 bit is set it will be set in r1 - jz r1,0,pos; test if sign = 0, e.g., r0 bit 0 is 0 - src r0,1,1,1; shift r0 logical left 1 bit - src r0,1,0,1; shift r0 logical right – sets sign bit to 0
\hl{I}
Instruction format: 0...5 Operation Code (OP), 6 7 General Purpose Register (R), 8 9 Index Register (I), 10 Indirect Bit (I), 11 ... 15 Address - a binary number also referred to as immediate
\hl{I}
Effective Address (EA) = contents of Address field plus contents of the index register specified in the IX field – here it is 10 + 2 or 13
\colorbox{Cyan}{0525 Lecture 3 Instruction Set Architecture} ISA is that portion of the machine visible to the assembly level programmer or to the compiler writer
\hl{I}
Challenges: Silicon Real Estate, Cost, Expandability, Legacy Support, Complexity
\hl{I}
The encoding of these instructions is one of the major tasks in instruction set design and requires very careful thought
\hl{I}
Instruction must specify: Which operation to perform, Where to find the operands (registers, memory, immediate, stack, other), Where to store the result (registers, memory, immediate, stack, other), Location of next instruction (either implicitly or explicitly)
\hl{I}
Types of Instructions: Transfer of control: jumps, calls | Arithmetic/Logical – add, subtract, multiply, divide, sqrt, log, sin, cos, ….; also, Boolean comparisons | Load/Store: load a value from memory or store a value to memory | Stack instructions: push/pop | System: traps, I/O, halt, no-operation | Floating Point Arithmetic | Decimal | String operations | Multimedia operations: Intel’s MMX and Sun’s VIS
\hl{I}
Multiple ways to add C = B + A: Stack Push A Push B Add Pop C | Accumulator Load A Add B Store C | Register (Register-memory) Load R1 A Add R1 B Store C R1 | Register (load-store) Load R1 A Load R2 B Add R3 R1 R2 Store C R3
\hl{I}
Stack Architecture: Need Top of Stack register, Stack Limit register, Pros Good code density (implicit operand addressing top of stack, Low hardware requirements, Easy to write a simple compiler for stack architectures, Cons, Stack becomes the bottleneck, Little ability for parallelism or pipelining, Data is not always at the top of stack when needed, so additional instructions are needed
\hl{I}
Accumulator Architectures: One or more accumulators (masquerade as GPRs), Pros Very low hardware requirements, Easy to design and understand, Cons, Accumulator becomes the bottleneck, Little ability for parallelism or pipelining, High memory traffic
\hl{I}
Memory-Memory Architectures: Operands fetched directly from memory into internal registers Pros, Requires fewer instructions (especially if 3 operands), Easy to write compilers for (especially if 3 operands), Cons Very high memory traffic (especially if 3 operands), Variable number of clocks per instruction (especially if 2 operands) and indexing and indirect access and , With two operands, more data movements are required
\hl{I}
Register-Memory Architectures: Operands fetched from register and memory Pros Some data can be accessed without loading first, Instruction format easy to encode, Good code density, Cons Operands are not equivalent (poor orthogonality), Variable number of clocks per instruction (w/ indexing and indirect access and), May limit number of registers 
\hl{I}
Load-Store/Register-Register Architectures: Operands fetched only from registers which are pre-loaded Pros Simple, fixed length instruction encoding, Instructions take similar number of cycles, Relatively easy to pipeline, Cons Higher instruction count , Not all instructions need three operands, Dependent on good compiler
\hl{I}
Addressing Types: Register Direct Addressing: The Register contains the operand, Direct Addressing to Main Memory: Instruction contains the Main Memory Address, Indirect Addressing To Main Memory: Instruction contains an address in main memory whose contents are the address of the operand
\hl{I}
Compilation Steps: Parsing intermediate representation, Jump Optimization, Loop Optimizations, Register Allocation, Code Generation assembly code, Common SubExpression, Procedure in-lining, Constant Propagation, Strength Reduction, Pipeline Scheduling 
\hl{I}
ISA Metrics: Orthogonality, completeness, regularity, streamlined design, ease of compilation
\hl{I}
ISA Design Space: number of explicit operands, operand storage, effective address, type and size of operands, operations.
\colorbox{Cyan}{0601 Lecture 4 Memory Systems} Memory System Design: Key Ideas: The Principle of Locality: Programs access a relatively small portion of the address space at any instant of time. Instructions and data both exhibit spatial and temporal locality. Temporal locality: If a particular instruction or data item is used now, there is a good chance that it will be used again in the near future. Spatial locality: If a particular instruction or data item is used now, there is a good chance that the instructions or data items that are located in memory immediately following or preceding this item will soon be used. Therefore, it is a good idea to move such instruction and data items that are expected to be used soon from slow memory to fast memory (cache). BUT! This is prediction, and therefore will not always be correct – depends on the extent of locality.
\hl{I}
Memory interleaving divides memory into banks as shown below. Addresses are distributed across the banks – 4-way interleaving is depicted. Interleaving allows simultaneous access to words in memory if the words are in separate banks. As we will see, this may conflict with caching
\hl{I}
You can think of computer memory as being one big array of data. The address serves as an array index. Each address refers to one word of data.
\hl{I}
Cache Performance: Memory access time = cache hit time or cache miss rate times miss penalty. To improve performance: reduce memory time  => we need to reduce hit time, miss rate and miss penalty. As L1 caches are in the critical path of instruction execution, hit time is the most important parameter. When one parameter is improved, others might suffer. Misses: Compulsory miss: always occurs first time. Capacity miss: reduces with increase in cache size. Conflict miss: reduces with level of associativity. Types: Instruction or Data Cache: 1-way or 2-way. Data Cache: write through and write-back
\hl{I}
Advantages of unified caches: Balance the load between instruction and data fetches depending on the dynamics of the program execution; Design and implementation are cheaper. Advantages of split caches (Harvard Architectures): Competition for the cache between instruction processing (which fetches from instruction cache) and execution functional units (which fetch from data cache) is eliminated. Instruction fetch can proceed in parallel with memory access from the execution unit.
\hl{I}
Cache Writing: A Write Buffer is needed between the Cache and Memory. Processor: writes data into the cache and the write buffer
Memory controller: write contents of the buffer to memory. Write buffer is just a FIFO: Typical number of entries: 4. Q. Why a write buffer ? So CPU doesn’t stall. Why a buffer, why not just one register? Bursts of writes are common. Are Read After Write (RAW) hazards an issue for write buffer? Yes! Drain buffer before next read, or send read 1st after check write buffers. Write: Need to update upper cache(s) and main memory whenever a store instruction modifies L1 cache. Write Hit: the item to be modified is in L1. Write Through: as if no L1, write also to L2. Write Back: set a Dirty Bit, and update L2 before replacing the block.
Although write through is an inefficient strategy, most L1s and some upper level caches follow this approach that read hit time is not affected due to complicated logic to update dirty bit. Write Miss: the item to be modified is not in L1. Write allocate: exploit locality, and bring the block to L1. Write no-allocate: do not fetch the missing block.
\hl{I}
Direct Mapped Cache: A memory block is mapped into a unique cache line, depending on the memory address of the respective block.
A memory address is considered to be composed of three fields: the least significant bits (2 in our example) identify the byte within the block; [assume four bytes/block]. the rest of the address (22 bits in our example) identify the block in main memory; for the cache logic, this part is interpreted as two fields: 2a. the least significant bits (14 in our example) specify the cache line; 2b. the most significant bits (8 in our example) represent the tag, which is stored in the cache together with the line. Advantages: simple and cheap; the tag field is short; only those bits have to be stored which are not used to address the cache (compare with the following approaches); access is very fast. Disadvantage: a given block fits into a fixed cache location
a given cache line will be replaced whenever there is a reference to another memory block which fits to the same line, regardless what the status of the other cache lines is. This can produce a low hit ratio, even if only a very small part of the cache is effectively used.
\hl{I}
Two Way Set Associative Cache: A block can be placed in a restricted set of places, or cache block frames. A set is a group of block frames in the cache. A block is first mapped onto the set and then it can be placed anywhere within the set.  The set in this case is chosen by:  (Block address)  MOD  (Number of sets in cache). Set associative mapping tries to eliminate the main shortcoming of direct mapping a certain flexibility is given concerning the line to be replaced when a new block is read into the cache. Cache hardware is more complex for set associative mapping than for direct mapping. In practice 2 and 4-way set associative mapping are used with very good results.
\hl{I}
Full Associative Cache: A block can be placed anywhere in cache. Lookup hardware for many tags can be large and slow
\hl{I}
Cache Optimizations: Fast Hit via Small and Simple Caches, Index tag memory and thereafter compare takes time | Fast Hit via Way Prediction, Make set-associative caches faster | Fast Hit via Trace Cache, Pack multiple non-contiguous basic blocks into one contiguous trace cache line | Increase Cache Bandwidth by Pipelining, Pipeline cache access to maintain bandwidth, but higher latency | Increasing Cache Bandwidth, Non-blocking cache or  lockup-free cache allow data cache to continue to supply cache hits during a miss requires out-of-order execution CPU | Increase Cache Bandwidth via Multiple Banks, Divide cache into independent banks that can support simultaneous accesses | Early Restart/Critical Word First to reduce miss penalty | Merging Write Buffer to Reduce Miss Penalty, Write buffer to allow processor to continue while waiting to write to memory | Reducing Misses By Compiler Optimizations, Instructions Reorder procedures in memory so as to reduce conflict misses, Profiling to look at conflicts
\colorbox{Cyan}{0606 Lecture 6 I/O Systems} I/O Types: I/O-mapped I/O uses special instructions to transfer data between the computer system and the outside world. Memory-mapped I/O uses special memory locations in the normal address space of the CPU to communicate with real-world devices. Direct Memory Access (DMA) is a special form of memory-mapped I/O where the peripheral device reads and writes data in memory without going through the CPU.
\hl{I}
Memory-Mapped I/O: A memory-mapped peripheral device is connected to the CPU's address and data lines exactly like memory. Principle Advantage: the CPU can use any instruction that accesses memory to transfer data between the CPU and a memory-mapped I/O device. Principle Disadvantage: they consume addresses in the memory map.
\hl{I}
I/O Mapped IO: I/O-mapped input/output uses special instructions to access I/O ports. Principle  Advantage: peripheral devices mapped to this area do not consume space in the memory address space. Principle Disadvantage:  it is quite small.
Although most peripheral devices only use a couple of I/O addresses (and most use fewer than 16-bit  I/O addresses), a few devices, like video display cards, can occupy millions of different I/O locations (e.g., three bytes for each pixel on the screen).
\hl{I}
Direct Memory Access: For very high-speed I/O devices the CPU may be too slow when processing this data a byte (or word or double word) at a time. Such devices generally have an interface to the CPU/Memory bus so they can directly read and write memory. Even if the CPU must halt and wait for the DMA operation to complete, the I/O is still much faster since many of the bus operations during I/O or memory-mapped I/O consist of instruction fetches or I/O port accesses which are not present during DMA operations. A typical DMA controller consists of a pair of counters and other circuitry that interfaces with memory and the peripheral device.
One of the counters serves as an address register. This counter supplies an address on the address bus for each transfer. The second counter specifies the number of transfers to complete. Each time the peripheral device wants to transfer data to or from memory, it sends a signal to the DMA controller. The DMA controller places the value of the address counter on the address bus.
At the same time, the peripheral device places data on the data bus (if this is an input operation) or reads data from the data bus (if this is an output operation). After a successful data transfer, the DMA controller increments its address register and decrements the transfer counter. This process repeats until the transfer counter decrements to zero. 
\hl{I}
Interrupts vs. Traps: I/O devices and the CPU can execute concurrently. Each device controller is in charge of a particular device type. Each device controller has a local buffer. CPU moves data from/to main memory to/from local buffers I/O is from the device to local buffer of controller. Device controller informs CPU that it has finished its operation by causing an interrupt. Determines which type of interrupt has occurred: polling, vectored interrupt system. Separate segments of code determine what action should be taken for each type of interrupt. A trap is a software-generated interrupt cause by: A user program error (e.g. division by zero) A user request for a service by the OS
\hl{I}
Types of Buses: processor-memory buses: short and high speed. I/O buses : long and have many devices connected to them. Backplane buses: allow processors, memory, and I/O devices to coexist on single bus
\hl{I}
Synchronous/Asynchronous Buses: Synchronous: clock in control lines. can be implemented easily in finite state machine. Every device on bus must run at the same clock rate. Because of clock skew, either short and fast, or long and slow processor/memory usually synchronous. Asynchronous- not clocked: can accommodate wide variety of devices. handshaking protocol: read-request data-ready, acknowledge. handshaking slow, so can use synchronizer. asynchronous scales better with technology changes and can support a wider variety of device response speeds, but increased overhead
\hl{I} Bus Issues: clocking, is bus clocked. switching, when control of bus is acquired and released. arbitration, deciding who gets the bus next.
\hl{I} Hard Disk Timing: seek position head over proper track seek time (min, max, avg). | Rotational delay/latency - after head reached correct track, time to wait for the desired sector to rotate under the read/write head average wait is half way around disk ~8.3ms . smaller diameter can spin at higher rates with less power consumption. | Transfer time - time to transfer a block of bits (sector) - function of sector size (2-15 MB/sec). | Disk controller - handles control of disk and transfer between disk and memory. Controller time - overhead because of disk controller.
\colorbox{Cyan}{0608 Lecture MIPS} Step 1: Instruction Fetch: Use PC to get instruction and put it in the Instruction Register. Add 4 to the PC and make that value available for selection. Instruction $=$ Memory $[P C]$; PCAdderOut =$\mathrm{PC}+4$
\hl{I}
Step 2 Instruction Decode and Register Fetch: Read registers rs and rt in case we need them. Compute the branch address in case the instruction is a branch Readdata1 = Reg[IR[25-21]]; Readdata2 = Reg[IR[20-16]]; Since inputs are available AddAluResult = PCAdderOut + sign-extend(IR[15-0])<<2;	We aren't setting any control lines based on the instruction type (we are busy "decoding" it in our control logic)
\hl{I}
Step 3 (instruction dependent) Execute: ALU is performing one of three functions, based on instruction type Memory Reference: ALUresult = Readdata1 + sign-extend(IR[15-0]); R-type: ALUresult = Readdata1 op Readdata2; Branch: if (Readdata1==Readdata2*) PC = AdderALUResult else PC = PCAdderOut  note that mux does this, simulate the mux (muxout = Branch and Zero Flag)
\hl{I}
Step 4 (R-type or memory-access): Loads and stores access memory Data Memory Readdata = Memory[ALUresult] Memory[ALUresult] = Readdata2; simulate the mux. R-type instructions finish	Reg[IR[15-11]] = ALUResult;  simulate the mux
\hl{I}
Step 5 Write-back step: Note that this step only occurs for memory read. The mux goint into the register file WriteRegister input determines which bits of the instruction to take.
Reg[IR[20-16]]= ReadData output of memory, again simulate that mux and the mux feeding the register file.
\colorbox{Cyan}{0608 Lecture Pipelines} Improve performance by increasing instruction throughput, Here we separate the instruction cycle events.  Stage length is driven by longest stage.
\hl{I}
Processor Cycle – the time it takes to move an instruction one step down the pipeline. All stages must be complete and ready to proceed at the same time. Designer must balance the length of each pipeline state, since they all must be ready to proceed at the same time, the processor cycle time is that time sufficient to handle the longest pipeline state execution.
\hl{I}
Pipelining in MIPS: All instructions are the same length and simple in format. At completion of fetch, no decisions to be made which would significantly vary the time length of the pipeline stage. The simple formats with the fields always in the same place simplifies instruction decode and register fetch.  Fixed-field decoding. Memory operands appear only in loads and stores
The ALU computes addresses for memory operands. It does not also have to be reused since no arithmetic operations are performed on data in a load or store, and no address computation is performed for an arithmetic operation. What makes it hard? structural hazards:  suppose we had only one memory. control hazards:  need to worry about branch instructions. data hazards:  an instruction depends on a previous instruction
\hl{I}
Have compiler guarantee no hazards. Where do we insert the “nops”? sub	\$2, \$1, \$3 and \$12, \$2, \$5 or	\$13, \$6, \$2 add	\$14, \$2, \$2	sw	\$15, 100(\$2)
\hl{I}
Forwarding: Use temporary results, don’t wait for them to be written. Register file forwarding to handle read/write to same register. ALU forwarding. Load word can still cause a hazard: an instruction tries to read a register following a load instruction that writes to the same register. Thus, we need a hazard detection unit to “stall” the load instruction. We can stall the pipeline by keeping an instruction in the same stage. Stall by letting an instruction that won’t write anything go forward.
\hl{I}
Branch Hazards: When we decide to branch, other instructions are in the pipeline! We are predicting “branch not taken”
need to add hardware for flushing instructions if we are wrong.
\hl{I}
Dynamic Scheduling: The hardware performs the “scheduling” hardware tries to find instructions to execute out of order execution is possible. speculative execution and dynamic branch prediction
\colorbox{Cyan}{0613 Lecture Branch Prediction} Reducing Branch Costs: The frequency of branches and jumps demands that we also attack stalls arising from control dependencies. As we are able to add parallel and multiple parallel units, branching becomes a constraining factor. On an n-issue processor, branches will arrive n times faster
\hl{I}
Dynamic Branch Prediction: Simplest scheme. A small memory indexed by the lower portion of the address of the branch instruction. Includes a bit that says whether the branch was taken recently or not. No other tags. Useful only to reduce the branch delay when it its longer than the time to compute the possible target PCs. Since we only use low order bits, some other branch instruction could have set the tag. The prediction is a hint that is assumed to be correct, if it turns out wrong, the prediction bit is inverted and stored back. The 1 bit scheme has a shortcoming. Even if a branch is almost always taken, we will usually predict incorrectly twice, rather than once, when it is not taken. Consider a loop branch that is taken nine times in a row then not taken.  What is the prediction accuracy for this branch, assuming the prediction bit for this branch remains in the prediction buffer. Mispredict on the the first and last predictions, as the loop branch was not taken on the first one as is set to 0.  Then on the last loop it will not be taken and the prediction will be wrong again. Down to 80\% accuracy here. To remedy this situation, 2 bit branch prediction schemes are often used. A prediction must miss twice before it is changed. A specialization of a more general scheme that has a n-bit saturating counter for each entry in the prediction buffer. With $n$ bits,we can take on the values 0 to $2^{\mathrm{n}}-1$. When the counter is $>=1 / 2$ of its max value, branch is predicted as taken. Count is incremented on a taken branch and decremented on a not taken one 2 bits work almost as well as larger numbers
\hl{I}
Correlating Branch Predictors: Branch predictors that use the behavior of other branches to make a prediction are called correlating predictors or two level predictors. MIPS Code DSUBUI R3,R1,\#2 | BNEZ R3,L1; branch b1(aa!=2) | DADD	R1,R0,R0; aa=0 | L1:	DSUBUI R3,R2,\#2 | BNEZ R3,L2; branch b2 (bb!=2) | DADD R2,R0,R0; bb=0 | L2:	DSUBU R3,R1,R2 | BEQZ R3,L3; branch b3(aa==bb) | Branch b3 is correlated with branches b1 and b2 – if branches b1 and b2 are both not taken then b3 will be taken since they are equal. If b1 is not taken then b2 will not be taken A 1 bit predictor initialized does not have the capability to take advantage of this.
\hl{I}
Branch Target Buffers: Reduce penalty in our 5 stage pipeline. Determine next instruction address to fetch by the end of IF 
We must know whether an instruction (not yet decoded) is a branch and, if so what the next PC should be. If at the end of IF we know the instruction is a branch and we know what the next PC should be, we have zero penalty. A branch prediction cache that stores the predicted address for the next instruction after a branch is called a branch target buffer or branch target cache. For the classic 5 stage pipeline, a branch prediction buffer is accessed during the ID cycle.  At the end of ID we know the branch target address (computed in ID), the fall through address (computed during IF), and the prediction. Thus by the end of ID we know enough to fetch the next predicted instruction. For a branch target buffer, we access the buffer during the IF stage using the instruction address of the fetched instruction (a possible branch) to index the buffer. If we get a hit, then we know the predicted instruction address at the end of the IF cycle, which is one cycle earlier than for the branch prediction buffer. This address is predicted and will be sent out before decoding the instruction.  It must be known whether the fetched instruction is predicted as a taken branch. 
\colorbox{Cyan}{0613 Lecture Exploiting Instruction Level Parallelism with Software Approaches} Exploiting parallelism among instructions. Finding sequences of unrelated instructions that can be overlapped in the pipeline. Separation of a dependent instruction from a source instruction by a distance in clock cycles equal to the pipeline latency of the source instruction. (Avoid the stall) The compiler works with a knowledge of the amount of available  ILP in the program and the latency's of the functional units within the pipeline. This couples the compiler, sometimes to the specific chip version, or at least requires the setting of appropriate compiler flags.
\hl{I}
MIPS code Loop:	L.D	F0,0(R1);F0 = array element | ADD.D	F4,F0,F2;add scalar in F2 | S.D	F4,0(R1);store back | DADDUI R1,R1,\#-8	decrement index | BNE R1,R2,Loop;R2 is precomputed so that 8(R2) is last value to be computed	
\hl{I}
Unrolling: The loop has been unrolled to 4 times its original 3 instructions to yield 12 instructions The only benefit here is that within the loop the loop overhead is a smaller percentage of code execution. We cannot move these instructions around except for the DADDUI Stalls still occur between the L.D and ADD.D in all 4 unrolls
\hl{I}
Loop: L.DF0,0(R1) | L.D	F6,-8(R1) | L.D	F10,-16(R1) | L.D F14,-24(R1) | ADD.D F4,F0,F2 | ADD.D	F8,F6,F2 | ADD.D F12,F10,F2 | ADD.D F16,F14,F2 | S.D	F4,0(R1) | S.D	F8,-8(R1) | DADDUI	R1,R1,\#-32 |S.D	F12,16(R1) | S.D F16,8(R1) | BNE R1,R2,Loop | Total of 14 clock cycles here for starting, 15 for branch to execute. Removal of anti-dependencies allows the compiler to move instructions and eliminate stalls
\hl{I}
Unrolling: Determine that it was legal to move the S.D after the DADDUI and BNE, and find the amount to adjust the S.D offset for remaining stores. Determine that unrolling the loop would be useful by finding that the loop iterations were independent, except for loop maintenance code. Use different registers to avoid unnecessary constraints that would be forced by using the same registers. Eliminate the extra test and branch instruction and adjust the loop termination and iteration code. Determine that the loads and stores can be interchanged by determining that the loads and stores from different iterations are independent. Schedule the code, preserving any dependencies
\hl{I}
Static Branch Prediction: Expectation is that branch behavior is highly predictable at compile time (can also be used to help dynamic predictors) It turns out that mis-prediction variance rates are large and that mis-predictions vary from between 9\% and 59\% for benchmarks. Look at this example, as stall for the DSUBU and BEQZ exists. Suppose this branch was almost always taken and that the value of R7 was not needed in the fall through.
\hl{I}
Prediction Schemes: Predict Branch as taken – Average misprediction equal to the untaken branch frequency which is about 34\% for the SPEC programs. For some programs the frequency of forward taken branches may be significantly less than 50\%. Predict on branch direction: Backward branches predicted as taken. Forward branches predicted as not taken. Misprediction rate still around 30\% to 40\%. Profile scheme: Based on information collected from earlier runs. Finds that an individual branch is often highly biased toward taken or untaken.
\hl{I}
VLIW Processors: First multiple issue processors requiring the instruction stream to be explicitly organized used wide instructions with multiple operations per instruction. Very Long Instruction Word (VLIW) 64, 128 or more bits wide. Multiple, independent functional units. Multiple operations packaged into one very long instruction, or require that issue packets are constrained. For discussion we assume the multiple instruction in one VLIW. No hardware needed to make instruction issue decisions. As maximum issue rate grows, the hardware to make the decisions becomes significantly more complex. Instructions might contain five operations, including one integer operation (which could be branch), two floating point operations, and two memory references. Set of fields for each functional unit, on the order of 16 to 24 bits per unit, yielding an instruction length of between 112 and 168 bits
\hl{I}
If iterations from loops are independent, then can get more ILP by taking instructions from different iterations
\colorbox{Cyan}{0613 Lecture Instruction Level Parallelism} An antidependence between instructions i and j occurs when instruction j writes a register or memory location that instruction i reads.  The original ordering must be preserved. An output dependence occurs when instruction i and instruction j write the same register or memory location, the order again must be preserved
\hl{I}
Data Hazards Three Types: RAW (read after write) – j tries to read a source before i writes it, so j incorrectly gets the old value. The most common type. Program order must be preserved. In a simple common static pipeline a load instruction followed by an integer ALU instruction that directly uses the load result will lead to a RAW hazard. WAW (write after write) – j tries to write an operand before it is written by i, with the writes ending up in the wrong order, leaving value written by i. Output dependence. Present in pipelines that write in more than one pipe or allow an instruction to proceed even when a previous instruction is stalled. In the classic example, WB stage is used for write back, this class of hazards avoided. If reordering of instructions is allowed this is a possible hazard. Suppose an integer instruction writes to a register after a floating point instruction does. WAR (write after read) – j tries to write an operand before it is read by i, so i incorrectly gets the new value. Antidependence. Cannot occur in most static pipelines – note that reads are early in ID and writes late in WB
\hl{I}
Preserving exception behavior means that any changes in the ordering of instruction execution must not change how exceptions are raised in the program
\hl{I}
Preserving Data Flow: This means preserving the actual flow of data values between instructions  that produce results and those that consume them. Branches make data flow dynamic, since they allow the source of data for a given instruction to come from many points  
\colorbox{Cyan}{0613 Lecture 8 Instruction Level Parallelism} Reorder Buffer (ROB) If instruction write results in program order, register or memory always gets the correct values. Reorder Buffer (ROB): re-order the out-of-order instructions at the time of writing (commit time) to program order. If the same instruction goes wrong, handle it at the time of commit – just flush the instruction afterwards. Instruction cannot write register or memory immediately after execution, so ROB also buffers the results
\hl{I}
Reservation Stations that hold instructions ready for execution (but only one functional unit to execute each class of instructions)
\hl{I}
Tomasulo Algorithm: At instruction issue, register specifiers (names) for the operand locations are renamed to the exact locations (e.g., physical registers) holding the operands. Values can exist in reservation stations or register file
to eliminate WARs, copy register values to reservation stations. Issue—get instruction from FP Op Queue. Condition: a free RS (Reservation Station) at the required FU (Functional Unit) Actions: (1) decode the instruction (2) allocate a RS and ROB entry (3) do source register renaming (4) do destination register renaming (5) read register file (6) dispatch the decoded \& renamed instruction to RS and ROB | Execution—operate on operands (EX) Condition: At a given FU, At least one instruction is ready Action: select a ready instruction and send it to the FU. | Write result—finish execution (WB = Write Buffer)
Condition: At a given FU, some instruction finishes FU execution Actions: (1) FU writes to CDB (Cache Data Buffer), broadcast to all RSs \& to ROB (2) FU broadcast tag (ROB index) to all RS (3) de-allocate the RS Note: no register status update at this time | Commit—update register with reorder result. Condition: ROB is not empty and ROB head instruction has finished execution. Actions if no misprediction/exception: (1) write result to register/memory (2) update register status (3) de-allocate the ROB entry. Actions if with misprediction/exception: flush the pipeline, e.g. (1) flush IFQ (Instruction Fetch Queue) (2) clear register status (3) flush all RS and reset FU (4) reset ROB. 
\colorbox{Cyan}{0615 Lecture 5 Instruction Level Parallelism and Its Dynamic Exploitation}
Hardware Based Speculation: Combination of 3 key activities | Dynamic branch prediction to choose which instructions to execute| Speculation to allow the execution of instructions before the control dependencies are resolved Must be able to undo effects of an incorrectly speculated sequence | Dynamic scheduling to deal with scheduling of different combinations of basic blocks. A particular approach is an extension of Tomasulo’s algorithm. Extend the hardware to support speculation. Separate the bypassing of results among instructions from the actual completion of an instruction, allowing an instruction to execute and to bypass its results to other instructions without allowing the instruction to perform any updates that cannot be undone. When an instruction is no longer speculative, it is committed. (An extra step in the instruction execution sequence allows it to update the register file or memory – “instruction commit”. Allow instructions to execute out of order but force them to commit in order to prevent irrevocable actions such as updating state or taking an exception. Add a commit phase (to our 5 stage pipeline as an example)
Requires changes to the sequence and additional set of hardware buffers that hold the results of instructions that have finished but not committed. The hardware buffer is called a ReOrder Buffer (ROB)
\hl{I}
Reorder Buffer (ROB): Provides additional registers in the same way as the reservation stations. Holds result of instruction between the time the operation associated with the instruction completes and the time the instruction commits. It therefore is a source of operands for instructions. With speculation, however, the register file is not updated until the instruction commits. Entries containing four fields | Instruction Type: Indicates whether the instruction is a branch, a store or a register operation | Destination: Supplies the register number or the memory address where the results should be written | Value: Holds the value of the instruction result until the instruction commits | Ready: Indicates that the instruction has completed execution and the value is ready. In use with Tomasulo ROBs completely replace the store buffers. Stores occur in two steps with the second step done by instruction commit. Results now tagged with ROB number rather than reservation station number (in this implementation)
\hl{I}
Instruction Execution With ROBs: Issue:  Get an instruction from instruction queue.  Issue if there is an empty reservation station and an empty slot in the ROB; send the operands to the reservation station if they are available in either the register or the ROB.  Update the control entries to indicate the buffers are in use; the number of the ROB allocated for the result being sent to the reservation station so that the number can be used to tag the result when it is placed on the CDB. If either all reservation stations or ROB is full, then instruction issue is stalled until both have available entries.  This stage is also called dispatch in a dynamically scheduled processor | Execute: If one or more operands is not yet available, monitor the CDB while waiting for the register to be computed. (Checks for RAW hazards) When both operands are available at a reservation station, execute the operation. Stores need only have the base register available at this step since execution for a store is the effective address calculation. | Write Result: When the result is available, write it on the CDB (with the ROB tag) and from the CDB to the ROB as well as to any reservation stations waiting for this result (they are watching for the tag also). Mark the reservation station as available. If the value to be stored is available, it is written to the value field of the ROB entry for store. If the value to be stored is not available, monitor CDB until that value is broadcast, at which time the value of the ROB entry of the store is updated. | Commit (3 cases): Committing instruction is a not a branch with incorrect prediction or a store (normal commit). Instruction reaches the head of the ROB and its result is present in the buffer, at this point the processor updates the register with the result and removes the instruction from the ROB. Committing instruction is a branch with incorrect prediction. Speculation was wrong, ROB is flushed and execution is restarted at the correct successor of the branch. Committing instruction is a store. Same as for normal.
\colorbox{Cyan}{0615 Lecture 5 Overcoming Data Hazards with Dynamic Scheduling and Tomasulo}
Tomasulo’s Approach - Background: RAW hazards – avoided by execution of an instruction only when its operands are available.
WAR and WAW hazards – eliminated by register renaming. All destination registers renamed including those with pending read or write for an earlier instruction DIV.D F0,F2,F4 ADD.D \& SUB.D has an antidependence | ADD.D F6,F0,F8 F8 must be used by ADD.D before SUB.D writes it or | S.D $\mathbf{F 6 , 0}(\mathrm{R} 1)$ | SUB.D F8,F10,F14 ADD.D must finish with R6 before S.D writes | MUL.D F6,F10,F8 if ADD.D finishes later than MUL.D |
\hl{I}
Assume 2 temporary registers S & T. S allows MUL.D to finish before ADD.D – removes F8. T allows SUB.D to finish before ADD.D. Any subsequent uses of F8 must be replaced by T. DIV.D F0,F2,F4 | ADD.D S,F0,F8 | S.D $\quad S, 0(R 1)$ | SUB.D T,F10,F14 | MUL.D F6,F10,T |
\hl{I}
Register renaming is provided by reservation stations. Buffer operands of instructions waiting to issue. Fetches and buffers an. operand as soon as it is available, eliminating need to get it from a register. Pending instructions designate the reservation station that will provide their input. As instructions are issued, the register specifiers for pending operands are renamed to the names of the reservation station. When successive writes to a register overlap in execution, only the last one is used to update the register. There can be more reservation stations than real registers. Hazard detection and execution control are distributed. Information held in the reservation stations at each functional unit determine when an instruction can begin execution at that unit. Results are passed directly to functional units from the reservation station where they are buffered
Common results bus (also called common data bus – CDB) that allows all units waiting for an operand to be loaded at once
In pipelines with multiple execution units and issuing multiple instructions per clock, more than one results bus will be needed.
\hl{I}
Instruction Execution in this Pipeline: Issue: Get the instruction from the head of the instruction queue, which is maintained in FIFO order. If there is a matching reservation station that is empty, issue the instruction to the station with the operand values, if they are currently in registers. If there is not an empty reservation station, then there is a structural hazard and the instruction stalls until a station or buffer is freed.  If the operands are not in the registers, keep track of the functional units that will produce the operands. REGISTERS RENAMED, WAR AND WAW HAZARDS ELIMINATED | Execute: If not all operands available, monitor the common data bus while waiting for the instruction to be completed.  When operand becomes available, it is placed into the corresponding reservation station. When all operands are available, operation can be executed at the corresponding functional unit. Delaying execution until all operands available, RAW hazards eliminated. Several instructions could become ready in the same clock cycle for the same functional unit – unit will have to choose. For floating point unit reservation stations, choice can be arbitrary (we are producing register results here). Load and store ( choosing when multiple instructions are ready) – two steps. Compute effective address when the base register is available. Effective address is then placed in the load or store buffer. Load/Store Loads in load buffer execute as soon as memory unit is available
Stores in the store buffer wait for the value that is to be stored before being sent to the memory unit
Loads and stores are maintained in program order through the effective address calculation. Preservation of exception behavior
No instruction is allowed to initiate execution until all branches that precede the instruction in program order have completed (this could be relax to say that no instruction will be allowed to cause an exception until all branches that precede the instruction in program order have completed; we will see this later). Processor must know that branch prediction was correct. Exception can be recorded but not actually raise it until appropriate time. | Write Result: When the result of the instruction is available, write it on the Common Data Bus and from there into the destination registers and into any reservation stations (including store buffers) waiting for this result. Stores write data to memory during this step.
\hl{I}
Reservation Stations:In the Tomasulo scheme, the tags refer to the buffer or unit that will produce the result.   Register names are discarded when an instruction issues to a reservation station. Each reservation station has seven fields |
Op – The operation to perform on source operands S1 and S2. | Qj, Qk – The reservation stations that will produce the corresponding source operand ( a value of 0 indicates that the operand is already available in Vj or Vk or is unnecessary). | Vj, Vk – The value of the source operands.  Only one of the V field or the Q field is valid for each operand.  For  loads, the Vk field is used to hold the offset field. | A – Used to hold information for the memory address calculation for a load or store – immediate field initially stored here, then EA. | Busy – Indicates that this reservation station and its accompanying functional unit are occupied
\hl{I}
Register file & Load-Store Buffers: The register file has one additional field, Qi. Qi – The number of the reservation station that contains the operation whose result should be stored into this register.  If the value is blank (or 0) no currently active instruction is computing a result destined for this register, meaning that the value is simply the register contents. The load and store buffers each have a field,  A. A – holds the result of the effective address once the first step of execution has been completed.
\colorbox{Cyan}{0622 Lecture 9 Vector Operations} Vector registers: Each vector register is a fixed-length bank holding a single vector. Usually comprised of normal general-purpose registers and floating-point registers. They can provide data as input to the vector functional units, as well as compute addresses.
\hl{I}
Vector functional units: Fully pipelined and can start a new operation on every clock cycle.
\hl{I}
Vector load-store unit: loads or stores a vector to or from memory.
\hl{I}
Vector Length Control: A vector has a natural length determined by the length of the vector registers.
\hl{I} Two Types of Vector Processors: Vector-Register Processors: All vector operations (except load and store) occur in the vector registers. Vector counterpart of a load-store architecture All major vector computers (Cray machines, NEC SX/2 ~ SX/5, Fujitsu VP200, etc.) | Memory-Memory Processors: All vector operations are memory to memory. CDC vector computers: CDC 203, CDC 205, TI ASC All are obsolete!
\hl{I}
Vector Processor: A scalar processor. Scalar register file. Scalar functional units (arithmetic, load/store, etc) |
A vector register file (a 2D register array). Each register is an array of elements, e.g. 32 registers with 32 64-bit elements per register. MVL = maximum vector length = max \# of elements per register | A set of pipelined vector functional units: Integer, FP, load/store, etc. Sometimes vector and scalar units are combined (share ALUs). | Three types of addressing Unit stride
Contiguous block of information in memory. Fastest: always possible to optimize this| Non-unit (constant) stride Harder to optimize memory system for all possible strides. Prime number of data banks makes it easier to support different strides at full bandwidth | Indexed (gather-scatter) Vector equivalent of register indirect. Good for sparse arrays of data. Increases number of programs that vectorize.
\hl{I}
Vector Pipeline: Consider the steps involved in a floating-point addition on a vector machine with IEEE Arithmetic hardware. The exponents of the two floating-point numbers to be added are compared to find the number with the smallest magnitude. The significands of the number with the smaller magnitude is shifted so that the exponents of the two numbers agree. The significands are added. The result of the addition is normalized. Checks are made to see if any floating-point exceptions occurred during the addition, such as overflow. Rounding occurs.
\hl{I}
Vector Start-up Time:  A measure of the latency in starting up the vector pipeline. The number of clock cycles required prior to the generation of the first result. The start-up time adds a considerable overhead for small value of N. The effect of start-up time  is negligible for large value of N. To maintain an initiation rate of one word fetched/store per clock, the memory must be able to meet this rate. Usually done by interleaving memory in banks.
\hl{I}
Maximum vector length  (MVL) issues: What to do when the application vector length is not exactly maximum vector length  (MVL)?
Vector-length (VL) register controls the length of any vector operation, including a vector load or store. Set it before performing any vector operation VADD.VV with VL=10 is equivalent to for (i=0; i<10; i++) V1[i] = V2[i]+V3[i] VL can be anything from 0 to MVL. Problem: Vector registers have finite length. Solution: Break loops into pieces that fit in registers, “Stripmining”. Vector Length modulo VL /= 0!! So, do short piece first, then do rest with length VL. EX: Suppose VL = 64. We have a vector that is 264, which is mod 8. So, process a vector length 8, then four vectors of length 64. Problem: All computations have some scalar components, e.g., non-vectorizable. Solution: Separate scale from vector computations (by hand; but maybe automatically)
\hl{I}
Characteristics of Vectorizable Code: Vectorization can only be done within a DO/FOR loop; it must be the innermost loop. It is crucial to ensure that there are sufficient iterations in the DO loop to offset the start-up time overhead. Put as much work as possible into a vectorizable statement to provide more opportunities for concurrent operations. There is a limit to vectorization because a compiler may not vectorize the code if it is too complicated. Exercise: How do you vectorize a WHILE loop?? The existence of certain operations in the DO loop may prevent the compiler from converting the entire, or part of the DO loop for vector processing: vectorization inhibitors include subroutine calls, recursion, references to external functions, and any input/output statements (which are actually system calls). These types of vector inhibitors can be removed by: expanding the function. in-lining subroutines at the point of reference.
\hl{I}
Vector Stride: Suppose adjacent elements of the vector are not sequential in memory do 10 i = 1,100 -> do 10 j = 1,100 -> A(i,j) = 0.0 -> do 10 k = 1,100 | A(i,j) = A(i,j)+B(i,k)*C(k,j). Either B or C accesses not adjacent (800 bytes between). stride: distance separating elements that are to be merged into a single vector (caches do unit stride) => LVWS (load vector with stride) instruction. Strides => can cause bank conflicts (e.g., stride = 32 and 16 banks)
\hl{I}
Vector Chaining: Suppose: MULV V1,V2,V3 | ADDV	V4,V1,V5 | chaining: vector register (V1) is not as a single entity but as a group of individual registers, then pipeline forwarding can work on individual elements of a vector. Flexible chaining: allow vector to chain to any other active vector operation => more read/write ports, e.g. pass the result from one vector operation to another vector operation. As long as enough HW, increases convoy size
\hl{I}
Vector Register Bypassing: LV $\mathrm{v} 1$ | MULV $v 3, v 1, v 2$ | ADDV $v 5, v 3, v 4$
\hl{I}
Vector Conditional Execution: Problem: Want to vectorize loops with conditional code: for $(i=0 ; i<\mathrm{N} ; i++)$ | if $(A[i]>0)$ then | $A[i]=B[i] ;$ | Solution: Add vector mask (or flag) registers. vector version of predicate registers, 1 bit per element. and maskable vector instructions. vector operation becomes NOP at elements where mask bit is clear. Code example:
CVM \# Turn on all elements | $L V \vee A, r A \quad \#$ Load entire $A$ vector | SGTVS.D VA, F0 # Set bits in mask register where $A>0$ | $L V v A, r B \quad$ # Load $B$ vector into $A$ under mask | $S V v A, r A \quad \#$ Store A back to memory under mask
\hl{I}
Vectors w/ Sparse Matrices: Suppose: do	100 i = 1,n | (outside do) A(K(i)) = A(K(i)) + C(M(i)) | gather (LVI) operation takes an index vector and fetches data from each address in the index vector. This produces a “dense” vector in the vector registers. After these elements are operated on in dense form,  the sparse vector can be stored in expanded form by a scatter store (SVI), using the same index vector. Can't be figured out by a compiler since it can't know elements distinct, no dependencies
Use CVI to create index 0, 1xm, 2xm, ..., 63xm
\colorbox{Cyan}{0627 Lecture 10 High Performance Computing}
Parallel Computing: Simultaneous use of multiple processors - all components of a single architecture - to solve a task. Typically processors identical, single user (even if machine is multi-user).
\hl{I}
Distributed Computing: Use of a network of processors, each capable of being viewed as a computer in its own right, to solve a problem. Processors  may be heterogeneous, multi-user, and usually individual tasks are assigned  to individual processors.
\hl{I}
Concurrent Computing: Distributed + Parallel
\hl{I}
Michael Flynn’s Hardware Taxonomy: SI:	Single Instruction Stream (a) All processors execute the same instruction in the same cycle. Instruction may be conditional in Multiprocessors, control processor issues the instruction | MI: Multiple Instruction Stream (c) Different processors may be simultaneously executing different instructions | SD: Single Data Stream (d)
All processors operate on the same data item (e.g., copies of it) at the same time | MD: Multiple Data Stream (b) Different processors may be simultaneously operating on different data items. Multiplying a coefficient vector by a data vector (e.g., in filtering) $y[I]:=c[I] \times x[I], 0 \leq i<n$
\hl{I}
Single Instruction Multiple Data (SIMD): Single (main) processor. Vector operations $\rightarrow$ One vector instruction $=$ many machine instructions on the data stream. Implemented as: Pipelined vector units or array of processors
\hl{I}
Multiple Instruction Single Data (MISD): Multiple Processors. Single Data stream is operated by multiple instruction streams. Pipeline Architectures and Systolic Arrays
\hl{I}
Multiple Instruction Multiple Data (MIMD): Multiple Processors,Multiple instruction streams operate on multiple data streams. Multi processors and Multi computers falls into this category.
\hl{I}
Processor Coupling: Tightly Coupled System: Tasks and/or processors communicate in a highly synchronized fashion | Communicates through a common shared memory Shared memory system | Loosely Coupled System: Tasks or processors do not communicate in a synchronized fashion. Communicates by message passing packets. Overhead for data exchange is high. Distributed memory system
\hl{I}
Granularity of Parallelism: Coarse-grain A task is broken into a handful of pieces, each of which is executed by a powerful processor. Processors may be heterogeneous. Computation/communication ratio is very high Example: BBN Butterfly | Medium-grain Tens to few thousands of processors typically running the same code. Computation/communication ratio is often hundreds or more. Intel Paragon XP, Touchstone Series | Fine-grain Thousands to perhaps millions of small pieces, executed by very small, simple processors or through pipelines. Processors typically have instructions broadcasted to them. Compute/communicate ratio often near unity. Example: Thinking Machines CM-1, CM-2, CM-200
\hl{I}
Memory Architextures: Shared (Global) Memory A Global Memory Space accessible by all processors. Processors may also have some local memory | Distributed (Local, Message-Passing) Memory. All memory units are associated with processors. To retrieve information from another processor's memory a message must be sent there. | Uniform Memory : All processors take the same time to reach all memory locations | Non-uniform (NUMA) Memory: Processors have varying access patterns to shared memory
\hl{I}
Operating System Master-Slave Multiprocessors: OS mostly runs on a single fixed CPU. User-level applications run on the other CPUs. All system calls are passed to the Master CPU for processing. Very little synchronization required. Single to implement. Single centralized scheduler to keep all processors busy. Memory can be allocated as needed to all CPUs. Issues: Master CPU becomes the bottleneck.
\hl{I} 
Interconnection Topologies: Shared Bus:All processors (and memory) are connected to a  common bus or busses. Memory access is fairly uniform, but not very scalable. A collection of signal lines that carry module-to-module communication. Data highways connecting several digital system elements. Can handle only one data transmission at a time. Can be easily expanded by connecting additional processors to the shared bus, along with the necessary bus arbitration circuitry.
\hl{I}
Multiport Memory: Has multiple sets of address, data, and control pins to allow simultaneous data transfers to occur. CPU and DMA controller can transfer data concurrently. A system with more than one CPU could handle simultaneous requests from two different processors. Does not scale well because of explosion of number of busses. Memory Module Control Logic. Each memory module has control logic. Resolve memory module conflicts via fixed priority among CPUs. Requests to read from and write to the same memory location simultaneously. Advantages Multiple paths -> high transfer rate. Disadvantages Multiple copies of Memory control logic Large number of connections
\colorbox{Cyan}{0629 Lecture Cache Coherency Snooping} Caches serve to: Increase bandwidth versus bus/memory. Reduce latency of access. Valuable for both private data and shared data. What about cache consistency?
\hl{I}
Coherency: Informally: “Any read must return the most recent write”. Too strict and too difficult to implement Better: “Any write must eventually be seen by a read” All writes are seen in proper order (“serialization”). Two rules to ensure this: “If P writes x and P1 reads it, P’s write will be seen by P1 if the read and write are sufficiently far apart”. Writes to a single location are serialized: seen in one order. Latest write will be seen. Otherwise could see writes in illogical order
\hl{I}
Snooping Solution (Snoopy Bus): Send all requests for data to all processors. Processors snoop to see if they have a copy and respond accordingly. Requires broadcast, since caching information is at processors. Works well with bus (natural broadcast medium). Dominates for small scale machines (most of the market).
\hl{I}
Basic Snoopy Protocols: Write Invalidate Protocol: Multiple readers, single writer. Write to shared data:  an invalidate is sent to all caches which snoop and invalidate any copies. Read Miss: Write-through: memory is always up-to-date. Write-back: snoop in caches to find most recent copy. Write Broadcast Protocol (typically write through): Write to shared data: broadcast on bus, processors snoop, and update any copies. Read miss: memory is always up-to-date. Write serialization: bus serializes requests! Bus is single point of arbitration. Write Invalidate versus Broadcast: Invalidate requires one transaction per write-run. Invalidate uses spatial locality: one transaction per block. Broadcast has lower latency between write and read. Write Invalidate versus Broadcast: Invalidate requires one transaction per write-run. Invalidate uses spatial locality: one transaction per block. Broadcast has lower latency between write and read
\hl{I}
Example Snoopy Protocol: Invalidation protocol, write-back cache. Each block of memory is in one state: Clean in all caches and up-to-date in memory (Shared). OR Dirty in exactly one cache (Exclusive). OR Not in any caches. Each cache block is in one state (track these): Shared : block can be read. OR Exclusive : cache has only copy, its writeable, and dirty. OR Invalid : block contains no data. Read misses: cause all caches to snoop bus. Writes to clean line are treated as misses.
\hl{I}
Implementation Complications: Write Races: Cannot update cache until bus is obtained. Otherwise, another processor may get bus first, and then write the same cache block! Two step process: Arbitrate for bus Place miss on bus and complete operation. If miss occurs to block while waiting for bus, handle miss (invalidate may be needed) and then restart. Split transaction bus: Bus transaction is not atomic: can have multiple outstanding transactions for a block. Multiple misses can interleave, allowing two caches to grab block in the Exclusive state. Must track and prevent multiple misses for one block. Must support interventions and invalidations.
\hl{I}
Implementing Snooping Caches: Multiple processors must be on bus, access to both addresses and data. Add a few new commands to perform coherency, in addition to read and write. Processors continuously snoop on address bus. If address matches tag, either invalidate or update. Since every bus transaction checks cache tags, could interfere with CPU just to check: Solution 1: duplicate set of tags for L1 caches just to allow checks in parallel with CPU. Solution 2: L2 cache already duplicate, provided L2 obeys inclusion with L1 cache. Block size, associativity of L2 affects L1. Bus serializes writes, getting bus ensures no one else can perform memory operation. On a miss in a write back cache, may have the desired copy and its dirty, so must reply. Add extra state bit to cache to determine shared or not. Add 4th state (MESI)
\colorbox{Cyan}{0629 Lecture Snooping Protocol Directory Protocol Synchronization Consistency}
Larger MPs: Separate Memory per Processor. Local or Remote access via memory controller. 1 Cache Coherency solution: non-cached pages. Alternative: directory per cache that tracks state of every block in every cache. Which caches have a copies of block, dirty vs. clean, ... Info per memory block vs. per cache block? PLUS: In memory => simpler protocol (centralized/one location)
MINUS: In memory => directory is ƒ(memory size) vs. ƒ(cache size). Prevent directory as bottleneck? distribute directory entries with memory, each keeping track of which Procs have copies of their blocks.
\hl{I}
Directory Protocol: Similar to Snoopy Protocol: Three states Shared: ≥ 1 processors have data, memory up-to-date. Uncached (no processor has it; not valid in any cache). Exclusive: 1 processor (owner) has data; memory out-of-date. |  In addition to cache state, must track which processors have data when in the shared state (usually bit vector, 1 if processor has copy). Keep it simple(r): Writes to non-exclusive data => write miss. Processor blocks until access completes. Assume messages received and acted upon in order sent. No bus and don’t want to broadcast: Interconnect no longer single arbitration point all messages have explicit responses. Terms: typically 3 processors involved. Local node where a request originates. Home node where the memory location of an address resides. Remote node has a copy of a cache block, whether exclusive or shared. Example messages on next slide: P = processor number, A = address
\hl{I}
State Transition Diagram for an Individual Cache Block in a Directory Based System: States identical to snoopy case; transactions very similar. Transitions caused by read misses, write misses, invalidates, data fetch requests. Generates read miss & write miss msg to home directory. Write misses that were broadcast on the bus for snooping => explicit invalidate & data fetch requests.
Note: on a write, a cache block is bigger, so need to read the full cache block
\hl{I}
State Transition Diagram for the Directory: Same states & structure as the transition diagram for an individual cache. 2 actions: update of directory state & send messages to satisfy requests. Tracks all copies of memory block. Also indicates an action that updates the sharing set, Sharers, as well as sending a message.
\hl{I}
Example Directory Protocol: Message sent to directory causes two actions: Update the directory. More messages to satisfy request.
Block is in Uncached state: the copy in memory is the current value; only possible requests for that block are: Read miss: requesting processor sent data from memory \& requestor made only sharing node; state of block made Shared. Write miss: requesting processor is sent the value \& becomes the Sharing node. The block is made Exclusive to indicate that the only valid copy is cached. Sharers indicates the identity of the owner. Block is Shared => the memory value is up-to-date: Read miss: requesting processor is sent back the data from memory \& requesting processor is added to the sharing set. Write miss: requesting processor is sent the value. All processors in the set Sharers are sent invalidate messages, \& Sharers is set to identity of requesting processor. The state of the block is made Exclusive. 
\hl{I}
Implementing a Directory: We assume operations atomic, but they are not; reality is much harder; must avoid deadlock when run out of buffers in network Optimizations: read miss or write miss in Exclusive: send data directly to requestor from owner vs. 1st to memory and then from memory to requestor
\colorbox{Cyan}{0629 Sample Final}
\colorbox{Orange}{Superscalar Architectures}
\textbf{Q.} a. Supplemental figure 2 has an illustration of a Tomasulo example discussed in class. How does the Tomasulo algorithm accomplish register renaming? 
\hl{I}
\textbf{A.} Registers are sources for operands. Registers are limited in number. Instructions are issued to reservation stations along with operand values. Reservation stations are then the sources of operands and computation results, with respect to registers.
\hl{I}
\textbf{Q.} b. Explain how the Tomasulo hardware makes maximum use of execution unit hardware, that is keeps the functional units busy.
\hl{I}
\textbf{A.} It issues instructions to reservation stations. Multiple instructions at any point in time may be waiting in reservation stations. Each reservation station may feed multiple functional units. The functional units operate in parallel. Functional units execute as soon as all operands are available.
\hl{I}
\textbf{Q.} c. How does a reorder buffer preserve exception behavior?
\hl{I}
\textbf{A.} The reorder buffer (ROB) stores exceptions from a reservation station execution and does not allow the exception until the offedning instruction is commited.
\colorbox{Orange}{Cache Coherency (Multiprocessors or Multicores)} 
\textbf{Q.} a. What types of cache coherency is used in massively parallel architectures and why?
\hl{I}
\textbf{A.} Bus snooping is not used. What is used is message passing distributed memory that is shared and a directory per processor that manages its local memory.
\hl{I}
\textbf{Q.} b. In a snooping protocol, why are write misses always put on the bus?
\hl{I}
\textbf{A.} This allows other processors caches to see the miss and invalidate their copies.
\colorbox{Orange}{Vector Processors}
\textbf{Q.} a. Explain why vector processors are much more efficient than compiler optimizations of loops and dynamic scheduling in scalar architectures.
\hl{I}
\textbf{A.} One instruction fetch causes multiple executions that would have been put in a loop in a scalar processor. Modern vector processors have optimized memory fetch/store hardware.
\hl{I}
\textbf{Q.} b. Briefly explain how vector units can accomplish forwarding?
\hl{I}
\textbf{A.} Using  a detection scheme similar to the forwarding unit in a scalar pipeline, outputs of functional units can be chained to inputs of other units based upon dependencies detected.
\colorbox{Orange}{Branch Prediction}
\textbf{Q.} a. How does a branch target buffer eliminate invalid stage executions? 
\hl{I}
\textbf{A.} In Microprocessor without Interlocked Pipelined Stages (MIPS) the predictive location of the next instruction to be executed as a result of the branch is known at the end of the instruction fetch (IF) stage of the branch instruction an can be routed to the program counter (PC).
\hl{I}
\textbf{Q.} b. Why does a 4 state (2 bit) branch predictor do much better than a 2 state (1 bit) branch predictor?
\hl{I}
\textbf{A.} A 1 bit predictor will invert the bit if the prediction is wrong and backward branches for loops will be mispredicted twice. A 2 bit predictor allows for more information about tendencies. A prediction must miss twice before it is changed. It performs better for backward branches of loops.
\hl{I}
\textbf{Q.} c. What penalties do we pay for incorrect predictions with a reorder buffer?
\hl{I}
\textbf{A.} At the commit of a branch if the prediction was incorrect, we clear all subsequent values in the reorder buffer (ROB). All computations beyond the incorrect branch must be redone.
\colorbox{Orange}{Static Code Optimization}
\textbf{Q.} Assume a standard Microprocessor without Interlocked Pipelined Stages (MIPS) architecture that we used in examples in class with branch determination and execution done in the second clock cycle for an instruction. Unroll the following loop once and shcedule (re-order) the instructions to maximize processor performance. Note that the loop is correct as is, and other possible optimizations may be possible. How many cycles are requiered to completely execute the unrolled loop once?
\hl{I}
loop: ld $r 6,0(r l)$
\hl{I}
ld $\mathbf{r} 2,0(\mathrm{r} 8)$
\hl{I}
add $r 6, r 6, r 2$
\hl{I}
st $r 6,0(r 1)$
\hl{I}
daddui r l.r1, #8
\hl{I}
bne $r$. $r$, loop
\hl{I}
\textbf{A.}
\colorbox{Orange}{Memory Hierarchy}
\textbf{Q.} a. What basic concept or concepts throughout the memory hierarchy dictate where copies of memory should be kept?
\hl{I}
\textbf{A.} Locality, temporal and spatial
\hl{I}
\textbf{Q.} b. What happens to cache performance when the number of blocks in a direct mapped cache are reduced?
\hl{I}
\textbf{A.} The cache is the same size, the blocks are larger, and compulsory misses are higher. Capacity misses go up and conflict misses go up.
\hl{I}
\textbf{Q.} c. For b, how does this affect other optimizations in instruction scheduling and execution units?
\hl{I}
\textbf{A.} The miss rate impacts execution in the pipeline and prefetch works better.


\end{document}  