\documentclass[fontsize=4pt]{scrartcl}
\usepackage{lmodern}
%\documentclass[10pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage[margin=0.25in]{geometry}        % See geometry.pdf to learn the layout options. There are lots.
%\geometry{letterpaper}                     % ... or a4paper or a5paper or ... 
\geometry{landscape}                        % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}                       % Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{esint}                          %for cyclic integrals
\usepackage{ amssymb } 
\usepackage[usenames, dvipsnames]{color}
\usepackage{multicol}
\usepackage{color,soul}
\usepackage{siunitx}                        % Scientific Notation
\usepackage{graphicx}

\def\rcurs{{\mbox{$\resizebox{.16in}{.08in}{\includegraphics{ScriptR}}$}}}
\def\brcurs{{\mbox{$\resizebox{.16in}{.08in}{\includegraphics{BoldR}}$}}}
\def\hrcurs{{\mbox{$\hat \brcurs$}}}

\begin{document}
\colorbox{YellowGreen}{Joe Crandall's CSCI 6461 Computer System Architecture}
\colorbox{Thistle}{Used Heavily}
\colorbox{Cyan}{Topic}
\colorbox{Orange}{SubTopic}
\colorbox{Aquamarine}{KNOWTHISMATH}
\colorbox{RubineRed}{Definition/Constant/Units}
\colorbox{Yellow}{break}
\colorbox{Cyan}{0518 Lecture 1 Introduction} 
Computer Architecture: Functional operation of the individual HW units within a computer system, and the flow of information and control among them.
\hl{I} 
Program Counter (PC): address of next instruction to be executed
\hl{I}
Condition Code (CC): set when arithmetic/logical operations are executed, four 1 bit elements, Overflow, Underflow, Divzero, EqualOrNot
\hl{I}
Instruction Register (IR): holds instruction to be executed
\hl{I}
Memory Address Register (MAR): holds the address of the word  to be fetched from memory
\hl{I}
Memory Buffer Register (MBR): holds the word just fetched from or the word to be last stored into memory
\hl{I}
general purpose registers (GPR0$\ldots$GPR3): mnemonically referred to, may be used as accumulators.
\hl{I}
machine fault register (MFR): contains the ID code of a machine fault after it occurs
\hl{I}
index registers: (X1$\ldots$X3): contains a base address that supports register addressing of memory.
\hl{I}
Instruction execution cyle: instruction fetch - obtain instruction from program storage, instruction decode - determine requiered actions and instruction size, operand fetch - locate and obtain operand data, execute - compute result value or status, result store - deposit results in storage for later use, next instruction - deterine sucessor instruction.
\colorbox{Cyan}{0523 Lecture 2 Basic System Design}
The simplest shifter is the shift register, which can shift by one position per clock cycle
\hl{I}
Performance, Resposne Time, Throughput, Execution Times
\hl{I}
Instruction Set Architecture, Processors, Memory, I/O Systems
\hl{I}
Finding the absolute value of an integer: abs - str	r0,0,$<$tempInt$>$; store r0 in $<$tempInt$>$, some location - ldr	r1,0,smask; mask for sign bit = 100 000 000 000 000 000 - and r1,r0; AND r1 and r0: if r0 bit is set it will be set in r1 - jz r1,0,pos; test if sign = 0, e.g., r0 bit 0 is 0 - src r0,1,1,1; shift r0 logical left 1 bit - src r0,1,0,1; shift r0 logical right – sets sign bit to 0
\hl{I}
Instruction format: 0...5 Operation Code (OP), 6 7 General Purpose Register (R), 8 9 Index Register (I), 10 Indirect Bit (I), 11 ... 15 Address - a binary number also referred to as immediate
\hl{I}
Effective Address (EA) = contents of Address field plus contents of the index register specified in the IX field – here it is 10 + 2 or 13
\colorbox{Cyan}{0525 Lecture 3 Instruction Set Architecture} ISA is that portion of the machine visible to the assembly level programmer or to the compiler writer
\hl{I}
Challenges: Silicon Real Estate, Cost, Expandability, Legacy Support, Complexity
\hl{I}
The encoding of these instructions is one of the major tasks in instruction set design and requires very careful thought
\hl{I}
Instruction must specify: Which operation to perform, Where to find the operands (registers, memory, immediate, stack, other), Where to store the result (registers, memory, immediate, stack, other), Location of next instruction (either implicitly or explicitly)
\hl{I}
Types of Instructions: Transfer of control: jumps, calls | Arithmetic/Logical – add, subtract, multiply, divide, sqrt, log, sin, cos, ….; also, Boolean comparisons | Load/Store: load a value from memory or store a value to memory | Stack instructions: push/pop | System: traps, I/O, halt, no-operation | Floating Point Arithmetic | Decimal | String operations | Multimedia operations: Intel’s MMX and Sun’s VIS
\hl{I}
Multiple ways to add C = B + A: Stack Push A Push B Add Pop C | Accumulator Load A Add B Store C | Register (Register-memory) Load R1 A Add R1 B Store C R1 | Register (load-store) Load R1 A Load R2 B Add R3 R1 R2 Store C R3
\hl{I}
Stack Architecture: Need Top of Stack register, Stack Limit register, Pros Good code density (implicit operand addressing top of stack, Low hardware requirements, Easy to write a simple compiler for stack architectures, Cons, Stack becomes the bottleneck, Little ability for parallelism or pipelining, Data is not always at the top of stack when needed, so additional instructions are needed
\hl{I}
Accumulator Architectures: One or more accumulators (masquerade as GPRs), Pros Very low hardware requirements, Easy to design and understand, Cons, Accumulator becomes the bottleneck, Little ability for parallelism or pipelining, High memory traffic
\hl{I}
Memory-Memory Architectures: Operands fetched directly from memory into internal registers Pros, Requires fewer instructions (especially if 3 operands), Easy to write compilers for (especially if 3 operands), Cons Very high memory traffic (especially if 3 operands), Variable number of clocks per instruction (especially if 2 operands) and indexing and indirect access and , With two operands, more data movements are required
\hl{I}
Register-Memory Architectures: Operands fetched from register and memory Pros Some data can be accessed without loading first, Instruction format easy to encode, Good code density, Cons Operands are not equivalent (poor orthogonality), Variable number of clocks per instruction (w/ indexing and indirect access and), May limit number of registers 
\hl{I}
Load-Store/Register-Register Architectures: Operands fetched only from registers which are pre-loaded Pros Simple, fixed length instruction encoding, Instructions take similar number of cycles, Relatively easy to pipeline, Cons Higher instruction count , Not all instructions need three operands, Dependent on good compiler
\hl{I}
Addressing Types: Register Direct Addressing: The Register contains the operand, Direct Addressing to Main Memory: Instruction contains the Main Memory Address, Indirect Addressing To Main Memory: Instruction contains an address in main memory whose contents are the address of the operand
\hl{I}
Compilation Steps: Parsing intermediate representation, Jump Optimization, Loop Optimizations, Register Allocation, Code Generation assembly code, Common SubExpression, Procedure in-lining, Constant Propagation, Strength Reduction, Pipeline Scheduling 
\hl{I}
ISA Metrics: Orthogonality, completeness, regularity, streamlined design, ease of compilation
\hl{I}
ISA Design Space: number of explicit operands, operand storage, effective address, type and size of operands, operations.
\colorbox{Cyan}{0601 Lecture 4 Memory Systems} Memory System Design: Key Ideas: The Principle of Locality: Programs access a relatively small portion of the address space at any instant of time. Instructions and data both exhibit spatial and temporal locality. Temporal locality: If a particular instruction or data item is used now, there is a good chance that it will be used again in the near future. Spatial locality: If a particular instruction or data item is used now, there is a good chance that the instructions or data items that are located in memory immediately following or preceding this item will soon be used. Therefore, it is a good idea to move such instruction and data items that are expected to be used soon from slow memory to fast memory (cache). BUT! This is prediction, and therefore will not always be correct – depends on the extent of locality.
\hl{I}
Memory interleaving divides memory into banks as shown below. Addresses are distributed across the banks – 4-way interleaving is depicted. Interleaving allows simultaneous access to words in memory if the words are in separate banks. As we will see, this may conflict with caching
\hl{I}
You can think of computer memory as being one big array of data. The address serves as an array index. Each address refers to one word of data.
\hl{I}
Cache Performance: Memory access time = cache hit time or cache miss rate times miss penalty. To improve performance: reduce memory time  => we need to reduce hit time, miss rate and miss penalty. As L1 caches are in the critical path of instruction execution, hit time is the most important parameter. When one parameter is improved, others might suffer. Misses: Compulsory miss: always occurs first time. Capacity miss: reduces with increase in cache size. Conflict miss: reduces with level of associativity. Types: Instruction or Data Cache: 1-way or 2-way. Data Cache: write through and write-back
\hl{I}
Advantages of unified caches: Balance the load between instruction and data fetches depending on the dynamics of the program execution; Design and implementation are cheaper. Advantages of split caches (Harvard Architectures): Competition for the cache between instruction processing (which fetches from instruction cache) and execution functional units (which fetch from data cache) is eliminated. Instruction fetch can proceed in parallel with memory access from the execution unit.
\hl{I}
Cache Writing: A Write Buffer is needed between the Cache and Memory. Processor: writes data into the cache and the write buffer
Memory controller: write contents of the buffer to memory. Write buffer is just a FIFO: Typical number of entries: 4. Q. Why a write buffer ? So CPU doesn’t stall. Why a buffer, why not just one register? Bursts of writes are common. Are Read After Write (RAW) hazards an issue for write buffer? Yes! Drain buffer before next read, or send read 1st after check write buffers. Write: Need to update upper cache(s) and main memory whenever a store instruction modifies L1 cache. Write Hit: the item to be modified is in L1. Write Through: as if no L1, write also to L2. Write Back: set a Dirty Bit, and update L2 before replacing the block.
Although write through is an inefficient strategy, most L1s and some upper level caches follow this approach that read hit time is not affected due to complicated logic to update dirty bit. Write Miss: the item to be modified is not in L1. Write allocate: exploit locality, and bring the block to L1. Write no-allocate: do not fetch the missing block.
\hl{I}
Direct Mapped Cache: A memory block is mapped into a unique cache line, depending on the memory address of the respective block.
A memory address is considered to be composed of three fields: the least significant bits (2 in our example) identify the byte within the block; [assume four bytes/block]. the rest of the address (22 bits in our example) identify the block in main memory; for the cache logic, this part is interpreted as two fields: 2a. the least significant bits (14 in our example) specify the cache line; 2b. the most significant bits (8 in our example) represent the tag, which is stored in the cache together with the line. Advantages: simple and cheap; the tag field is short; only those bits have to be stored which are not used to address the cache (compare with the following approaches); access is very fast. Disadvantage: a given block fits into a fixed cache location
a given cache line will be replaced whenever there is a reference to another memory block which fits to the same line, regardless what the status of the other cache lines is. This can produce a low hit ratio, even if only a very small part of the cache is effectively used.
\hl{I}
Two Way Set Associative Cache: A block can be placed in a restricted set of places, or cache block frames. A set is a group of block frames in the cache. A block is first mapped onto the set and then it can be placed anywhere within the set.  The set in this case is chosen by:  (Block address)  MOD  (Number of sets in cache). Set associative mapping tries to eliminate the main shortcoming of direct mapping a certain flexibility is given concerning the line to be replaced when a new block is read into the cache. Cache hardware is more complex for set associative mapping than for direct mapping. In practice 2 and 4-way set associative mapping are used with very good results.
\hl{I}
Full Associative Cache: A block can be placed anywhere in cache. Lookup hardware for many tags can be large and slow
\hl{I}
Cache Optimizations: Fast Hit via Small and Simple Caches, Index tag memory and thereafter compare takes time | Fast Hit via Way Prediction, Make set-associative caches faster | Fast Hit via Trace Cache, Pack multiple non-contiguous basic blocks into one contiguous trace cache line | Increase Cache Bandwidth by Pipelining, Pipeline cache access to maintain bandwidth, but higher latency | Increasing Cache Bandwidth, Non-blocking cache or  lockup-free cache allow data cache to continue to supply cache hits during a miss requires out-of-order execution CPU | Increase Cache Bandwidth via Multiple Banks, Divide cache into independent banks that can support simultaneous accesses | Early Restart/Critical Word First to reduce miss penalty | Merging Write Buffer to Reduce Miss Penalty, Write buffer to allow processor to continue while waiting to write to memory | Reducing Misses By Compiler Optimizations, Instructions Reorder procedures in memory so as to reduce conflict misses, Profiling to look at conflicts
\colorbox{Cyan}{0606 Lecture 6 I/O Systems} I/O Types: I/O-mapped I/O uses special instructions to transfer data between the computer system and the outside world. Memory-mapped I/O uses special memory locations in the normal address space of the CPU to communicate with real-world devices. Direct Memory Access (DMA) is a special form of memory-mapped I/O where the peripheral device reads and writes data in memory without going through the CPU.
\hl{I}
Memory-Mapped I/O: A memory-mapped peripheral device is connected to the CPU's address and data lines exactly like memory. Principle Advantage: the CPU can use any instruction that accesses memory to transfer data between the CPU and a memory-mapped I/O device. Principle Disadvantage: they consume addresses in the memory map.
\hl{I}
I/O Mapped IO: I/O-mapped input/output uses special instructions to access I/O ports. Principle  Advantage: peripheral devices mapped to this area do not consume space in the memory address space. Principle Disadvantage:  it is quite small.
Although most peripheral devices only use a couple of I/O addresses (and most use fewer than 16-bit  I/O addresses), a few devices, like video display cards, can occupy millions of different I/O locations (e.g., three bytes for each pixel on the screen).
\hl{I}
Direct Memory Access: For very high-speed I/O devices the CPU may be too slow when processing this data a byte (or word or double word) at a time. Such devices generally have an interface to the CPU/Memory bus so they can directly read and write memory. Even if the CPU must halt and wait for the DMA operation to complete, the I/O is still much faster since many of the bus operations during I/O or memory-mapped I/O consist of instruction fetches or I/O port accesses which are not present during DMA operations. A typical DMA controller consists of a pair of counters and other circuitry that interfaces with memory and the peripheral device.
One of the counters serves as an address register. This counter supplies an address on the address bus for each transfer. The second counter specifies the number of transfers to complete. Each time the peripheral device wants to transfer data to or from memory, it sends a signal to the DMA controller. The DMA controller places the value of the address counter on the address bus.
At the same time, the peripheral device places data on the data bus (if this is an input operation) or reads data from the data bus (if this is an output operation). After a successful data transfer, the DMA controller increments its address register and decrements the transfer counter. This process repeats until the transfer counter decrements to zero. 
\hl{I}
Interrupts vs. Traps: I/O devices and the CPU can execute concurrently. Each device controller is in charge of a particular device type. Each device controller has a local buffer. CPU moves data from/to main memory to/from local buffers I/O is from the device to local buffer of controller. Device controller informs CPU that it has finished its operation by causing an interrupt. Determines which type of interrupt has occurred: polling, vectored interrupt system. Separate segments of code determine what action should be taken for each type of interrupt. A trap is a software-generated interrupt cause by: A user program error (e.g. division by zero) A user request for a service by the OS
\hl{I}
Types of Buses: processor-memory buses: short and high speed. I/O buses : long and have many devices connected to them. Backplane buses: allow processors, memory, and I/O devices to coexist on single bus
\hl{I}
Synchronous/Asynchronous Buses: Synchronous: clock in control lines. can be implemented easily in finite state machine. Every device on bus must run at the same clock rate. Because of clock skew, either short and fast, or long and slow processor/memory usually synchronous. Asynchronous- not clocked: can accommodate wide variety of devices. handshaking protocol: read-request data-ready, acknowledge. handshaking slow, so can use synchronizer. asynchronous scales better with technology changes and can support a wider variety of device response speeds, but increased overhead
\hl{I} Bus Issues: clocking, is bus clocked. switching, when control of bus is acquired and released. arbitration, deciding who gets the bus next.
\hl{I} Hard Disk Timing: seek position head over proper track seek time (min, max, avg). | Rotational delay/latency - after head reached correct track, time to wait for the desired sector to rotate under the read/write head average wait is half way around disk ~8.3ms . smaller diameter can spin at higher rates with less power consumption. | Transfer time - time to transfer a block of bits (sector) - function of sector size (2-15 MB/sec). | Disk controller - handles control of disk and transfer between disk and memory. Controller time - overhead because of disk controller.
\colorbox{Cyan}{0608 Lecture MIPS} Step 1: Instruction Fetch: Use PC to get instruction and put it in the Instruction Register. Add 4 to the PC and make that value available for selection. Instruction $=$ Memory $[P C]$; PCAdderOut =$\mathrm{PC}+4$
\hl{I}
Step 2 Instruction Decode and Register Fetch: Read registers rs and rt in case we need them. Compute the branch address in case the instruction is a branch Readdata1 = Reg[IR[25-21]]; Readdata2 = Reg[IR[20-16]]; Since inputs are available AddAluResult = PCAdderOut + sign-extend(IR[15-0])<<2;	We aren't setting any control lines based on the instruction type (we are busy "decoding" it in our control logic)
\hl{I}
Step 3 (instruction dependent) Execute: ALU is performing one of three functions, based on instruction type Memory Reference: ALUresult = Readdata1 + sign-extend(IR[15-0]); R-type: ALUresult = Readdata1 op Readdata2; Branch: if (Readdata1==Readdata2*) PC = AdderALUResult else PC = PCAdderOut  note that mux does this, simulate the mux (muxout = Branch and Zero Flag)
\hl{I}
Step 4 (R-type or memory-access): Loads and stores access memory Data Memory Readdata = Memory[ALUresult] Memory[ALUresult] = Readdata2; simulate the mux. R-type instructions finish	Reg[IR[15-11]] = ALUResult;  simulate the mux
\hl{I}
Step 5 Write-back step: Note that this step only occurs for memory read. The mux goint into the register file WriteRegister input determines which bits of the instruction to take.
Reg[IR[20-16]]= ReadData output of memory, again simulate that mux and the mux feeding the register file.
\colorbox{Cyan}{0608 Lecture Pipelines} Improve performance by increasing instruction throughput, Here we separate the instruction cycle events.  Stage length is driven by longest stage.
\hl{I}
Processor Cycle – the time it takes to move an instruction one step down the pipeline. All stages must be complete and ready to proceed at the same time. Designer must balance the length of each pipeline state, since they all must be ready to proceed at the same time, the processor cycle time is that time sufficient to handle the longest pipeline state execution.
\hl{I}
Pipelining in MIPS: All instructions are the same length and simple in format. At completion of fetch, no decisions to be made which would significantly vary the time length of the pipeline stage. The simple formats with the fields always in the same place simplifies instruction decode and register fetch.  Fixed-field decoding. Memory operands appear only in loads and stores
The ALU computes addresses for memory operands. It does not also have to be reused since no arithmetic operations are performed on data in a load or store, and no address computation is performed for an arithmetic operation. What makes it hard? structural hazards:  suppose we had only one memory. control hazards:  need to worry about branch instructions. data hazards:  an instruction depends on a previous instruction
\hl{I}
Have compiler guarantee no hazards. Where do we insert the “nops”? sub	\$2, \$1, \$3 and \$12, \$2, \$5 or	\$13, \$6, \$2 add	\$14, \$2, \$2	sw	\$15, 100(\$2)
\hl{I}
Forwarding: Use temporary results, don’t wait for them to be written. Register file forwarding to handle read/write to same register. ALU forwarding. Load word can still cause a hazard: an instruction tries to read a register following a load instruction that writes to the same register. Thus, we need a hazard detection unit to “stall” the load instruction. We can stall the pipeline by keeping an instruction in the same stage. Stall by letting an instruction that won’t write anything go forward.
\hl{I}
Branch Hazards: When we decide to branch, other instructions are in the pipeline! We are predicting “branch not taken”
need to add hardware for flushing instructions if we are wrong.
\hl{I}
Dynamic Scheduling: The hardware performs the “scheduling” hardware tries to find instructions to execute out of order execution is possible. speculative execution and dynamic branch prediction
\colorbox{Cyan}{0613 Lecture Branch Prediction}

\colorbox{Cyan}{0613 Lecture Exploiting Instruction Level Parallelism with Software Approaches}

\colorbox{Cyan}{0613 Lecture Instruction Level Parallelism}

\colorbox{Cyan}{0613 Lecture 8 Instruction Level Parallelism}

\colorbox{Cyan}{0613 Lecture Pipelines}

\colorbox{Cyan}{0615 Lecture 5 Instruction Level Parallelism and Its Dynamic Exploitation}

\colorbox{Cyan}{0615 Lecture 5 Overcoming Data Hazards with Dynamic Scheduling}

\colorbox{Cyan}{0615 Lecture Tomasulo}

\colorbox{Cyan}{0622 Lecture 9 Vector Operations}

\colorbox{Cyan}{0627 Lecture 10 High Performance Computing}

\colorbox{Cyan}{0629 Lecture Cache Coherency Snooping}

\colorbox{Cyan}{0629 Lecture Snooping Protocol Directory Protocol Synchronization Consistency}

\colorbox{Cyan}{0629 Sample Final}
\colorbox{Orange}{Superscalar Architectures}
\textbf{Q.} a. Supplemental figure 2 has an illustration of a Tomasulo example discussed in class. How does the Tomasulo algorithm accomplish register renaming? 
\hl{I}
\textbf{A.} Registers are sources for operands. Registers are limited in number. Instructions are issued to reservation stations along with operand values. Reservation stations are then the sources of operands and computation results, with respect to registers.
\hl{I}
\textbf{Q.} b. Explain how the Tomasulo hardware makes maximum use of execution unit hardware, that is keeps the functional units busy.
\hl{I}
\textbf{A.} It issues instructions to reservation stations. Multiple instructions at any point in time may be waiting in reservation stations. Each reservation station may feed multiple functional units. The functional units operate in parallel. Functional units execute as soon as all operands are available.
\hl{I}
\textbf{Q.} c. How does a reorder buffer preserve exception behavior?
\hl{I}
\textbf{A.} The reorder buffer (ROB) stores exceptions from a reservation station execution and does not allow the exception until the offedning instruction is commited.
\colorbox{Orange}{Cache Coherency (Multiprocessors or Multicores)} 
\textbf{Q.} a. What types of cache coherency is used in massively parallel architectures and why?
\hl{I}
\textbf{A.} Bus snooping is not used. What is used is message passing distributed memory that is shared and a directory per processor that manages its local memory.
\hl{I}
\textbf{Q.} b. In a snooping protocol, why are write misses always put on the bus?
\hl{I}
\textbf{A.} This allows other processors caches to see the miss and invalidate their copies.
\colorbox{Orange}{Vector Processors}
\textbf{Q.} a. Explain why vector processors are much more efficient than compiler optimizations of loops and dynamic scheduling in scalar architectures.
\hl{I}
\textbf{A.} One instruction fetch causes multiple executions that would have been put in a loop in a scalar processor. Modern vector processors have optimized memory fetch/store hardware.
\hl{I}
\textbf{Q.} b. Briefly explain how vector units can accomplish forwarding?
\hl{I}
\textbf{A.} Using  a detection scheme similar to the forwarding unit in a scalar pipeline, outputs of functional units can be chained to inputs of other units based upon dependencies detected.
\colorbox{Orange}{Branch Prediction}
\textbf{Q.} a. How does a branch target buffer eliminate invalid stage executions? 
\hl{I}
\textbf{A.} In Microprocessor without Interlocked Pipelined Stages (MIPS) the predictive location of the next instruction to be executed as a result of the branch is known at the end of the instruction fetch (IF) stage of the branch instruction an can be routed to the program counter (PC).
\hl{I}
\textbf{Q.} b. Why does a 4 state (2 bit) branch predictor do much better than a 2 state (1 bit) branch predictor?
\hl{I}
\textbf{A.} A 1 bit predictor will invert the bit if the prediction is wrong and backward branches for loops will be mispredicted twice. A 2 bit predictor allows for more information about tendencies. A prediction must miss twice before it is changed. It performs better for backward branches of loops.
\hl{I}
\textbf{Q.} c. What penalties do we pay for incorrect predictions with a reorder buffer?
\hl{I}
\textbf{A.} At the commit of a branch if the prediction was incorrect, we clear all subsequent values in the reorder buffer (ROB). All computations beyond the incorrect branch must be redone.
\colorbox{Orange}{Static Code Optimization}
\textbf{Q.} Assume a standard Microprocessor without Interlocked Pipelined Stages (MIPS) architecture that we used in examples in class with branch determination and execution done in the second clock cycle for an instruction. Unroll the following loop once and shcedule (re-order) the instructions to maximize processor performance. Note that the loop is correct as is, and other possible optimizations may be possible. How many cycles are requiered to completely execute the unrolled loop once?
\hl{I}
loop: ld $r 6,0(r l)$
\hl{I}
ld $\mathbf{r} 2,0(\mathrm{r} 8)$
\hl{I}
add $r 6, r 6, r 2$
\hl{I}
st $r 6,0(r 1)$
\hl{I}
daddui r l.r1, #8
\hl{I}
bne $r$. $r$, loop
\hl{I}
\textbf{A.}
\colorbox{Orange}{Memory Hierarchy}
\textbf{Q.} a. What basic concept or concepts throughout the memory hierarchy dictate where copies of memory should be kept?
\hl{I}
\textbf{A.} Locality, temporal and spatial
\hl{I}
\textbf{Q.} b. What happens to cache performance when the number of blocks in a direct mapped cache are reduced?
\hl{I}
\textbf{A.} The cache is the same size, the blocks are larger, and compulsory misses are higher. Capacity misses go up and conflict misses go up.
\hl{I}
\textbf{Q.} c. For b, how does this affect other optimizations in instruction scheduling and execution units?
\hl{I}
\textbf{A.} The miss rate impacts execution in the pipeline and prefetch works better.


\end{document}  